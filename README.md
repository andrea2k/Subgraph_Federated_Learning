# Subgraph Federated Learning <!-- omit in toc -->

A repository for **synthetic subgraph-detection** benchmarking and **PNA** baselines on directed multigraphs.

This repository generates synthetic multigraphs with subgraph pattern labels, partitions them into federated subgraphs using Metis- and Louvain-based splitting strategies, and trains centralized or federated PNA-based models for financial crime detection.

## Table of Contents <!-- omit in toc -->

- [Synthetic Graph Generation](#synthetic-graph-generation)
  - [Label Tasks](#label-tasks)
  - [Default Generation Settings for Synthetic Graph](#default-generation-settings-for-synthetic-graph)
  - [How to Generate Synthetic Graph](#how-to-generate-synthetic-graph)
- [Federated Subgraph Partitioning](#federated-subgraph-partitioning)
  - [Community-Detection-Based Partitioning](#community-detection-based-partitioning)
    - [Original Splits (Equal-Sized Clients)](#original-splits-equal-sized-clients)
    - [Original Splits (Zipf-Skewed Clients)](#original-splits-zipf-skewed-clients)
    - [Label-Controlled Splits (Imbalance-Handled)](#label-controlled-splits-imbalance-handled)
    - [How to Generate Metis- and Louvain-based Splits](#how-to-generate-metis--and-louvain-based-splits)
  - [Pattern-Aware Federated Splits (Witness-Based)](#pattern-aware-federated-splits-witness-based)
    - [Motivation](#motivation)
    - [Client Subgraph Structure](#client-subgraph-structure)
    - [How to Generate Pattern-Aware Splits](#how-to-generate-pattern-aware-splits)
    - [Sanity Checking Pattern Dispersion Across Clients](#sanity-checking-pattern-dispersion-across-clients)
- [Principal Neighborhood Aggregation (PNA)](#principal-neighborhood-aggregation-pna)
  - [1. Baseline PNA (Full-Batch Training)](#1-baseline-pna-full-batch-training)
  - [2. PNA with Reverse Message Passing (Mini-Batch Training)](#2-pna-with-reverse-message-passing-mini-batch-training)
  - [3. Training Configuration](#3-training-configuration)
- [PNA Training Under Federated Setting](#pna-training-under-federated-setting)
  - [Federated Learning Configuration](#federated-learning-configuration)
    - [Federated Dataset Simulation](#federated-dataset-simulation)
    - [Federated Learning Hyperparameters](#federated-learning-hyperparameters)
- [Reproducibility](#reproducibility)

## Synthetic Graph Generation

This repository includes a **synthetic subgraph-detection dataset** used for benchmarking graph models for the pattern detection task. The graphs and labels are generated following the pseudocode and configurations described in [Provably Powerful Graph Neural Networks for Directed Multigraphs](https://arxiv.org/abs/2306.11586) (Egressy et al., 2023).

### Label Tasks

Each node is labeled for the presence of the following patterns (11 sub-tasks):

- `deg_in > 3`
- `deg_out > 3`
- `fan_in > 3`
- `fan_out > 3`
- `cycle2`
- `cycle3`
- `cycle4`
- `cycle5`
- `cycle6`
- `scatter_gather`
- `biclique`

---

### Default Generation Settings for Synthetic Graph

The default generation config (see the generator script `scripts/data/generate_synthetic.py`) follows the paper’s setup:

- Nodes `n = 8192`
- Average degree `d = 6`
- Radius parameter `r = 11.1`
- Directed multigraphs (for directed cycles)
- Generator: `chordal` / random-circulant-like
- One connected component per split (prevents data leakage)

---

### How to Generate Synthetic Graph

From the repository root, run:

```bash
python3 -m scripts.data.generate_synthetic
```

This command generates the synthetic pattern-detection graphs and saves the following files:

- `./data/train.pt`
- `./data/val.pt`
- `./data/test.pt`
- `./data/y_sums.csv` — positive-label counts per sub-task
- `./results/metrics/label_percentages.csv` — label percentages for sanity checking against the statistics reported by the original paper

## Federated Subgraph Partitioning

### Community-Detection-Based Partitioning

In the federated setting, each client is represented by a subgraph of the global synthetic graph. We use two community-detection–based partitioning techniques:

- **Metis:** balanced k-way graph partitioning
- **Louvain:** modularity-based community detection

Both follow the methodology of
[OpenFGL: A Comprehensive Benchmark for Federated Graph Learning](https://arxiv.org/abs/2408.16288) (Li et al., 2024), extended here for multi-task labels.

#### Original Splits (Equal-Sized Clients)

The default experimental setup uses **approximately equal-sized clients**. After detecting communities, we assign them to clients using a greedy bin-packing strategy, producing subgraphs with similar node counts. This provides a controlled and stable federated environment for evaluating performance differences between centralized and decentralized training.

#### Original Splits (Zipf-Skewed Clients)

To simulate more realistic financial crime settings with different client sizes, we additionally support **Zipf-skewed** splits. Communities are assigned to clients according to a Zipf-like distribution, producing:

- a few large clients,
- many small clients.

These splits model strongly **non-uniform client sizes**, common in real-world networks.

#### Label-Controlled Splits (Imbalance-Handled)

We also provide **label-controlled (imbalance-handled)** splits following the OpenFGL label-distribution clustering strategy. Communities are clustered based on their multi-task label distributions and grouped to explicitly reduce extreme label skew across clients. These splits are intended for **controlled benchmarking**, where label distributions and task difficulty are kept comparable across clients.

---

#### How to Generate Metis- and Louvain-based Splits

From the repository root:

```bash
python3 -m scripts.data.make_federated_splits
```

This produces **six federated split directories** (each containing per-client `.pt` files) **plus a client size summary** under `./data/`:

- `fed_louvain_splits/` — Louvain, equal-sized
- `fed_metis_splits/` — Metis, equal-sized
- `fed_louvain_splits_zipf_skewed/` — Louvain, Zipf-skewed
- `fed_metis_splits_zipf_skewed/` — Metis, Zipf-skewed
- `fed_louvain_imbalance_splits/` — Louvain, label-controlled (imbalance-handled)
- `fed_metis_imbalance_splits/` — Metis, label-controlled (imbalance-handled)

Additionally:

- `client_sizes.csv` — Number of nodes and edges for each client across all split types

The training script automatically selects the appropriate directory using:

```json
"partition_strategy": "<strategy_name>"
```

Examples:
`"metis original"`, `"louvain original skewed"`, `"metis imbalance"`

---

### Pattern-Aware Federated Splits (Witness-Based)

In addition to community-detection–based partitioning, this repository provides a **pattern-aware federated splitting strategy** that explicitly divides _structural subgraph patterns_ across clients.

Unlike Metis- or Louvain-based approaches, which operate purely on graph topology, this strategy uses **pattern witnesses**, the exact node sets that form each labeled subgraph instance (e.g., cycles, scatter–gather motifs, or bicliques), to guide client assignment.

#### Motivation

In realistic federated financial crime detection settings, illicit activity patterns are often **distributed across institutions** rather than localized within a single organization. Community-based partitioning can unintentionally concentrate entire subgraph patterns within a single client, making the federated task artificially easy.

The witness-based strategy addresses this by enforcing **instance-level pattern dispersion**:

> Nodes participating in the same structural pattern are, whenever possible, assigned to different clients.

This yields a federated dataset with **stronger non-IID structure** and a more realistic financial crime setting.

---

#### Client Subgraph Structure

Each client subgraph retains **all edges incident to at least one node owned by that client**. When an edge connects an owned node to a node assigned to a different client, the non-owned endpoint is included as a **ghost node**, ensuring that cross-client edges remain available for message passing.

As a result, each client subgraph contains both **owned nodes** and **ghost nodes**. Ghost nodes participate only in message passing, while **training loss and evaluation metrics are computed exclusively on owned nodes**.

---

#### How to Generate Pattern-Aware Splits

Running the synthetic data generation script:

```bash
python3 -m scripts.data.generate_synthetic
```

produces an additional federated split directory at `./data/fed_witness_splits/`.

This directory contains **pattern-aware federated splits** for each global graph split (`train/`, `val/`, `test/`). Each split directory has the following structure:

- `clients/client_XXXX.pt` — per-client subgraphs
- `node_to_client.pt` — node-to-client assignment
- `client_sizes.csv` — number of nodes and edges per client
- `witness_split_sanity.csv` — sanity check showing how often pattern instances are split across distinct clients

The training script automatically uses these **pattern-aware splits** when the federated configuration specifies:

```json
"partition_strategy": "partition aware"
```

---

#### Sanity Checking Pattern Dispersion Across Clients

To verify correct splitting behavior, the **dispersion statistics** are computed.

For each pattern type, the following metrics are reported:

- number of pattern instances evaluated,
- fraction of instances achieving maximal dispersion across clients,
- average number of distinct clients per pattern instance,
- worst-case (minimum) dispersion observed.

These statistics confirm that:

- small patterns (e.g., 2- and 3-cycles) are almost always perfectly split,
- larger patterns are consistently distributed across multiple clients,
- no pattern ends up entirely within a single client.

This provides strong empirical validation that the federated split respects **pattern-level heterogeneity**.

## Principal Neighborhood Aggregation (PNA)

This repository provides two implementations of the **Principal Neighborhood Aggregation (PNA)** model, one baseline version using standard message passing, and an enhanced version that incorporates **Reverse Message Passing**, **Ego IDs**, **Port IDs**, and **mini-batch neighborhood sampling** for scalable training.

Both implementations follow the PNA architecture introduced in
[Principal Neighbourhood Aggregation for Graph Nets](https://arxiv.org/abs/2004.05718) (Corso et al., 2020).

### 1. Baseline PNA (Full-Batch Training)

The baseline model uses the original PNAConv layers from PyTorch Geometric and is trained in the **full-batch** setting.

To train and evaluate the baseline model:

```bash
python3 -m scripts.training.train_pna_baseline
```

The baseline model:

- operates directly on the homogeneous directed multigraph,
- uses full-batch message passing over the entire graph,
- does **not** use Ego IDs or Port IDs,
- serves as the reference for evaluating all incremental adaptations.

---

### 2. PNA with Reverse Message Passing (Mini-Batch Training)

This extended version incorporates several adaptations designed to improve pattern detection in directed multigraphs:

- **Reverse Message Passing** (direction-aware PNA aggregation)
- **Heterogeneous graph transformation** (`fwd` and `rev` edge types)
- **Ego ID embeddings** (to preserve seed-identity across sampled mini-batches)
- **Port ID embeddings** (to encode in/out-port numbers)
- **Mini-batch neighborhood sampling** using PyG’s `NeighborLoader`
- **Configurable fanout per hop** (default: `[10, 4]`)

To train and evaluate this model:

```bash
python3 -m scripts.training.train_pna_reverse_mp
```

This version serves as the foundation for future **federated** extensions.

---

### 3. Training Configuration

Both PNA variants share the following core hyperparameters:

- **`hidden_dim = 64`**
  Dimensionality of node embeddings throughout the network.

- **`num_layers = 2`**
  Number of GNN layers in the model.

- **`dropout = 0.1`**
  Dropout rate applied during training to reduce overfitting.

- **`lr = 0.001`**
  Learning rate used by the Adam optimizer.

- **`weight_decay = 0.0001`**
  L2 regularization strength to prevent overfitting.

Additional hyperparameters apply to the extended PNA model with reverse message passing:

- **`batch_size = 32`**
  Number of seed nodes sampled per mini-batch.

- **`neighbors_per_hop = [10, 4]`**
  Number of neighbors sampled at each hop for scalable neighborhood expansion.

- **`ego_dim = 32`**
  Embedding dimension used to encode ego-node identity across batches.

- **`port_emb_dim = 8`**
  Embedding dimension for port IDs, capturing in-/out-port structural information.

- **`minority_class_weight = "auto"`**
  Class-weighting strategy. When set to _auto_, the loss function computes per-task positive-class weights from the training labels.

All configurations are available in `.configs/pna_configs.json` file.

## PNA Training Under Federated Setting

To train and evaluate the PNA model in the federated learning setting:

```bash
python3 -m scripts.training.train_federated_pna
```

### Federated Learning Configuration

The federated setting introduces additional hyperparameters governing both the **federated splits generation** and the **federated training procedure**. This section documents the default configuration used throughout the experiments, along with a brief rationale for each choice.

#### Federated Dataset Simulation

- **`num_clients = 32`**
  The 8192-node global graph is partitioned into 32 subgraphs, yielding approximately 256 nodes per client.
  This creates a **realistically challenging** federated scenario: clients are small enough to introduce non-IID behavior but large enough to support stable local training.

- **`louvain_resolution = 1.0`**
  Uses the default modularity resolution for Louvain community detection.

- **`metis_num_coms = 32`**
  The Metis partitioning strategy is configured to produce exactly 32 partitions, ensuring that **each client corresponds to one contiguous graph community**, which maximizes structural separation between clients.

#### Federated Learning Hyperparameters

- **`partition_strategy`**
  Selects the partitioning strategy used in the experiment.
  Supported options:
  `"partition aware"`, `"metis original"`, `"louvain original"`, `"metis original skewed"`, `"louvain original skewed"`, `"metis imbalance"`, `"louvain imbalance"`.

- **`global_epochs = 100`**
  The total number of global training rounds.

- **`local_epochs = 2`**
  Each client performs two passes over its local subgraph during every communication round.

- **`client_fraction = 1.0`**
  All clients participate in every communication round.

- **`algorithm`**
  Specifies the federated learning algorithm used in the experiment.
  Currently supported options:
  `"fedavg"`, `"fedprox"`.

All configurations are available in `.configs/fed_configs.json` file.

## Reproducibility

All datasets, federated splits, and training results in this repository are **fully reproducible**.

The entire pipeline uses a shared seed-derivation mechanism:

- A global `BASE_SEED` is defined in the config.
- Each script calls `set_seed(BASE_SEED)`.
- Task-specific seeds (e.g., `"train"`, `"val"`, `"louvain"`, `"metis"`) are deterministically obtained via `derive_seed(BASE_SEED, tag)`.

This ensures:

- **Synthetic graphs** (`train.pt`, `val.pt`, `test.pt`) are identical across runs.
- **Federated subgraph partitions** (Metis/Louvain, equal-sized, Zipf-skewed, LIS) are reproduced exactly.
- **Training runs** (both centralized and federated) are stable and repeatable, including model initialization, mini-batch sampling, and client sampling.

Changing the `BASE_SEED` produces a new, independent experiment instance while preserving internal consistency across all components.
